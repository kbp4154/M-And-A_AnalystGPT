{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b7db4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello this is inside the jupyter notebook\n"
     ]
    }
   ],
   "source": [
    "print(\"hello this is inside the jupyter notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db61effc-ab3c-40f4-8e58-f903460ea3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, M&A AnalystGPT!\n",
      "0.3.25\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello, M&A AnalystGPT!\")\n",
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5acd3c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 591 chunks\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "doc = fitz.open(\"amazon_10k_2024.pdf\")  # Adjust filename as needed\n",
    "text = \"\".join(page.get_text() for page in doc)\n",
    "chunks = [text[i:i+500] for i in range(0, len(text), 500)]\n",
    "print(f\"Created {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48a2d4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing amazon_10k_2024.pdf...\n",
      "Added 591 chunks from amazon_10k_2024.pdf\n",
      "Processing Tesla_10k_2024.pdf...\n",
      "Added 812 chunks from Tesla_10k_2024.pdf\n",
      "Processing ExxonMobil_10k_2024.pdf...\n",
      "Added 946 chunks from ExxonMobil_10k_2024.pdf\n",
      "Processing apple_10k_2024.pdf...\n",
      "Added 429 chunks from apple_10k_2024.pdf\n",
      "Processing Microsoft_10k_2024.pdf...\n",
      "Added 807 chunks from Microsoft_10k_2024.pdf\n",
      "Processing Alphabet_10k_2024.pdf...\n",
      "Added 742 chunks from Alphabet_10k_2024.pdf\n",
      "Processing JPMorgan_10k_2024.pdf...\n",
      "Added 2515 chunks from JPMorgan_10k_2024.pdf\n",
      "Processing Johnson&Johnson_10k_2024.pdf...\n",
      "Added 874 chunks from Johnson&Johnson_10k_2024.pdf\n",
      "Processing walmart_10k_2024.pdf...\n",
      "Added 775 chunks from walmart_10k_2024.pdf\n",
      "Processing Pepsico_10k_2024.pdf...\n",
      "Added 926 chunks from Pepsico_10k_2024.pdf\n",
      "Total chunks created: 9417\n",
      "Chunks saved to all_chunks.txt\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "\n",
    "pdf_files = [\n",
    "    \"amazon_10k_2024.pdf\",\n",
    "    \"Tesla_10k_2024.pdf\",\n",
    "    \"ExxonMobil_10k_2024.pdf\",\n",
    "    \"apple_10k_2024.pdf\",\n",
    "    \"Microsoft_10k_2024.pdf\",\n",
    "    \"Alphabet_10k_2024.pdf\",\n",
    "    \"JPMorgan_10k_2024.pdf\",\n",
    "    \"Johnson&Johnson_10k_2024.pdf\",\n",
    "    \"walmart_10k_2024.pdf\",\n",
    "    \"Pepsico_10k_2024.pdf\"\n",
    "]\n",
    "\n",
    "all_chunks = []\n",
    "for pdf_file in pdf_files:\n",
    "    print(f\"Processing {pdf_file}...\")\n",
    "    try:\n",
    "        doc = fitz.open(pdf_file)\n",
    "        text = \"\".join(page.get_text() for page in doc)\n",
    "        chunks = [text[i:i+500] for i in range(0, len(text), 500)]\n",
    "        all_chunks.extend(chunks)\n",
    "        print(f\"Added {len(chunks)} chunks from {pdf_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_file}: {e}\")\n",
    "\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "with open(\"all_chunks.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, chunk in enumerate(all_chunks, 1):\n",
    "        f.write(f\"Chunk {i}: {chunk}\\n\")\n",
    "print(\"Chunks saved to all_chunks.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61a3dc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 9417 documents\n"
     ]
    }
   ],
   "source": [
    "with open(\"all_chunks.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_chunks = [line.split(\": \", 1)[1].strip() for line in f if line.startswith(\"Chunk\")]\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "documents = [Document(page_content=chunk) for chunk in all_chunks]\n",
    "print(f\"Created {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d06bfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created\n",
      "Result 1: o a number of risks,...\n",
      "Result 2: Are Harmed by the Products We Sell or Manufacture...\n",
      "Result 3: , and loss of critical data, and could prevent us from accepting and fulfilling customer...\n",
      "Result 4: siness and Industry Risks...\n",
      "Result 5: s. Any such disruptions or issues may harm our brand and business....\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load chunks from all_chunks.txt\n",
    "with open(\"all_chunks.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_chunks = [line.split(\": \", 1)[1].strip() for line in f if line.startswith(\"Chunk\")]\n",
    "\n",
    "# Use a subset (e.g., 1,000) to avoid memory issues\n",
    "documents = [Document(page_content=chunk) for chunk in all_chunks[:1000]]\n",
    "print(f\"Created {len(documents)} documents\")\n",
    "\n",
    "# Custom embedding class for SentenceTransformer\n",
    "class SentenceTransformerEmbeddings:\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return self.model.encode(texts, convert_to_numpy=True).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode([text], convert_to_numpy=True)[0].tolist()\n",
    "\n",
    "# Initialize the custom embeddings\n",
    "embeddings = SentenceTransformerEmbeddings()\n",
    "\n",
    "# Create the vector store with the embeddings object\n",
    "vector_store = FAISS.from_documents(documents, embeddings)\n",
    "print(\"Vector store created\")\n",
    "\n",
    "# Perform the query using similarity_search_by_vector as a workaround\n",
    "query = \"What are the risks mentioned in Amazon's 10-K?\"\n",
    "query_embedding = embeddings.embed_query(query)  # Generate embedding for the query\n",
    "results = vector_store.similarity_search_by_vector(query_embedding, k=5)\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"Result {i}: {doc.page_content[:200]}...\")  # Print first 200 chars of each result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f5dfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing amazon_10k_2024.pdf...\n",
      "Added 591 chunks from amazon_10k_2024.pdf\n",
      "Processing Tesla_10k_2024.pdf...\n",
      "Added 812 chunks from Tesla_10k_2024.pdf\n",
      "Processing ExxonMobil_10k_2024.pdf...\n",
      "Added 946 chunks from ExxonMobil_10k_2024.pdf\n",
      "Processing apple_10k_2024.pdf...\n",
      "Added 429 chunks from apple_10k_2024.pdf\n",
      "Processing Microsoft_10k_2024.pdf...\n",
      "Added 807 chunks from Microsoft_10k_2024.pdf\n",
      "Processing Alphabet_10k_2024.pdf...\n",
      "Added 742 chunks from Alphabet_10k_2024.pdf\n",
      "Processing JPMorgan_10k_2024.pdf...\n",
      "Added 2515 chunks from JPMorgan_10k_2024.pdf\n",
      "Processing Johnson&Johnson_10k_2024.pdf...\n",
      "Added 874 chunks from Johnson&Johnson_10k_2024.pdf\n",
      "Processing walmart_10k_2024.pdf...\n",
      "Added 775 chunks from walmart_10k_2024.pdf\n",
      "Processing Pepsico_10k_2024.pdf...\n",
      "Added 926 chunks from Pepsico_10k_2024.pdf\n",
      "\n",
      "‚úÖ Total chunks created: 9417\n",
      "üìÑ Chunks saved to all_chunks.txt\n",
      "üìÑ Loaded 2000 documents\n",
      "‚è≥ Embedding and building vector store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector store created in 5.51 seconds\n",
      "üíæ Vector store saved to 'faiss_store'\n",
      "\n",
      "üß† Enter queries below (type 'quit' to exit):\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import fitz  # PyMuPDF for PDF processing\n",
    "import os\n",
    "import time\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Step 1: PDF Preprocessing and Chunking\n",
    "pdf_files = [\n",
    "    \"amazon_10k_2024.pdf\", \"Tesla_10k_2024.pdf\", \"ExxonMobil_10k_2024.pdf\",\n",
    "    \"apple_10k_2024.pdf\", \"Microsoft_10k_2024.pdf\", \"Alphabet_10k_2024.pdf\",\n",
    "    \"JPMorgan_10k_2024.pdf\", \"Johnson&Johnson_10k_2024.pdf\",\n",
    "    \"walmart_10k_2024.pdf\", \"Pepsico_10k_2024.pdf\"\n",
    "]\n",
    "\n",
    "all_chunks = []\n",
    "for pdf_file in pdf_files:\n",
    "    print(f\"Processing {pdf_file}...\")\n",
    "    try:\n",
    "        doc = fitz.open(pdf_file)\n",
    "        text = \"\".join(page.get_text() for page in doc)\n",
    "        chunks = [text[i:i+500] for i in range(0, len(text), 500)]\n",
    "        all_chunks.extend(chunks)\n",
    "        print(f\"Added {len(chunks)} chunks from {pdf_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {pdf_file}: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total chunks created: {len(all_chunks)}\")\n",
    "with open(\"all_chunks.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, chunk in enumerate(all_chunks, 1):\n",
    "        f.write(f\"Chunk {i}: {chunk}\\n\")\n",
    "print(\"üìÑ Chunks saved to all_chunks.txt\")\n",
    "\n",
    "# Step 2: Load Chunks\n",
    "with open(\"all_chunks.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_chunks = [line.split(\": \", 1)[1].strip() for line in f if line.startswith(\"Chunk\")]\n",
    "\n",
    "# ‚ö†Ô∏è TEMP: Reduce for testing - later increase to 5000 if memory allows\n",
    "documents = [Document(page_content=chunk) for chunk in all_chunks[:2000]]\n",
    "print(f\"üìÑ Loaded {len(documents)} documents\")\n",
    "\n",
    "# Step 3: Custom Embedding Class (with optional GPU)\n",
    "class SentenceTransformerEmbeddings:\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\", use_gpu=False):\n",
    "        device = \"cuda\" if use_gpu else \"cpu\"\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return self.model.encode(texts, convert_to_numpy=True).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode([text], convert_to_numpy=True)[0].tolist()\n",
    "\n",
    "# Step 4: Create or Load Vector Store\n",
    "embeddings = SentenceTransformerEmbeddings(use_gpu=False)\n",
    "vector_store_path = \"faiss_store\"\n",
    "\n",
    "if os.path.exists(vector_store_path):\n",
    "    vector_store = FAISS.load_local(vector_store_path, embeddings)\n",
    "    print(\"üìÇ Loaded vector store from disk\")\n",
    "else:\n",
    "    print(\"‚è≥ Embedding and building vector store...\")\n",
    "    start = time.time()\n",
    "    vector_store = FAISS.from_documents(documents, embeddings)\n",
    "    print(f\"‚úÖ Vector store created in {time.time() - start:.2f} seconds\")\n",
    "    vector_store.save_local(vector_store_path)\n",
    "    print(f\"üíæ Vector store saved to '{vector_store_path}'\")\n",
    "\n",
    "# Step 5: CLI Query Interface\n",
    "print(\"\\nüß† Enter queries below (type 'quit' to exit):\")\n",
    "while True:\n",
    "    query = input(\"\\nüîé Your query: \")\n",
    "    if query.lower() == 'quit':\n",
    "        break\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    results = vector_store.similarity_search_by_vector(query_embedding, k=5)\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"\\nResult {i}:\\n{doc.page_content[:300]}...\")\n",
    "\n",
    "# Step 6: Optional LangChain Agent Integration\n",
    "try:\n",
    "    from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "    tools = [\n",
    "        Tool(\n",
    "            name=\"Search10K\",\n",
    "            func=lambda q: [doc.page_content for doc in vector_store.similarity_search_by_vector(embeddings.embed_query(q), k=5)],\n",
    "            description=\"Search across 10-K filings.\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    agent = initialize_agent(tools, embeddings, agent_type=\"zero-shot-react-description\")\n",
    "    print(\"\\nü§ñ Agent initialized. Try: `agent.run('Compare Microsoft and Tesla risks')`\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è LangChain agents not installed. Run: pip install -U langchain-agents\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M&A AnalystGPT",
   "language": "python",
   "name": "manalystgpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
